{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this notebook we want to accomplish the following (this list will be updated):\n",
    "* Prepare the tweet_text so that we can use them in NLP algorithms. For example, we can:\n",
    "  * Make words in lower case (not hard so will do it later).\n",
    "  * Use Stemming (not hard so will do it later)\n",
    "  * Delete Punctuation/special characters (Maybe we can study the relevance of punctuation/special characters) and maybe it is better to no delete these. \n",
    "  * Create Functions to count punctuation and special characters.\n",
    "  * Create Function to count Hashtags, extract Hashtags, count Tags, extract Tags, count emojis, extract emojis.\n",
    "  * Assign a sentiment score to each Tweet using some pretrained NLP tool.\n",
    "* Study a way to process the location_geocode data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import emoji\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tw  = pd.read_csv(\"Data/CleanedData.csv\")\n",
    "df_geo = pd.read_csv(\"Data/location_geocode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will find/build tools to process text.\n",
    "\n",
    "#Create a list of hashtags in a given tweet\n",
    "def extract_hashtags(tweet):\n",
    "    return [w[1:] for w in tweet.split() if w.startswith('#') ]\n",
    "\n",
    "#Create a list of tags in a given tweet\n",
    "def extract_tags(tweet):\n",
    "    return [w[1:] for w in tweet.split() if w.startswith('@') ]\n",
    "\n",
    "def extract_emojis(tweet):\n",
    "    return [ch for ch in list(tweet) if ch in emoji.UNICODE_EMOJI['en']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['narendramodi', 'smritiirani']\n",
      "['üôè']\n"
     ]
    }
   ],
   "source": [
    "tweet_example = list(df_tw['full_text'].loc[[1]])[0]\n",
    "\n",
    "print(extract_hashtags(tweet_example))\n",
    "print(extract_tags(tweet_example))\n",
    "print(extract_emojis(tweet_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will used a pretrained Sentiment Analyzer (VADER) to assign polarity scores to our tweets. For simplicity we will only decide if the tweet is positive or negative. We will say that compud = 0 is negative. Although it my be a good idea to get neutral scores. Change of mind we will get neutral also using what this links is doing https://towardsdatascience.com/comparing-vader-and-text-blob-to-human-sentiment-77068cf73982. I feel should used a little value that pm 0.05\n",
    "\n",
    "tweet_example = list(df_tw['full_text'].loc[[11]])[0]\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(tweet_example)['compound']\n",
    "\n",
    "#The value number is a positive number <1 to determine the range of neutral tweets. The neutral tweet will be when -value<comput < value\n",
    "def get_sentiment_label(tweet,value):\n",
    "    polarity_info = sia.polarity_scores(tweet)\n",
    "    if polarity_info['compound'] >= value:\n",
    "        return 'positive'\n",
    "    elif polarity_info['compound'] < value and polarity_info['compound'] > -value:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Australia Votes Out Far-Right Lawmaker Egged By Teen  Dont egg vote.  https://t.co/KLN2EWRGk9',\n",
       " 'neutral',\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some Exmpples\n",
    "tweet_example = list(df_tw['full_text'].loc[[1343]])[0]\n",
    "tweet_example,get_sentiment_label(tweet_example,0.2), sia.polarity_scores(tweet_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here We are going to Prepare the data to do some visualization. We will include the following:\n",
    "# tweet, sentiment_label, emojis, hashtags, tags, likes, retweet, date\n",
    "\n",
    "tweets = dict(df_tw['full_text'])\n",
    "sentiment_label = {}\n",
    "emojis = {}\n",
    "hashtags = {}\n",
    "tag = {}\n",
    "likes = dict(df_tw['favorite_count'])\n",
    "retweet = dict(df_tw['retweet_count'])\n",
    "date = {}\n",
    "for i in df_tw.index:\n",
    "   date_temp = 'No Date'\n",
    "   if type(df_tw['created_at'].loc[i]) == str:\n",
    "      date_temp = datetime.strptime(df_tw['created_at'].loc[i], '%Y-%m-%d %H:%M:%S')\n",
    "   tweet_temp = list(df_tw['full_text'].loc[[i]])[0]\n",
    "   sentiment_label[i] = get_sentiment_label(tweet_temp,.2)\n",
    "   emojis[i] = extract_emojis(tweet_temp)\n",
    "   hashtags[i] = extract_hashtags(tweet_temp)\n",
    "   tag[i] = extract_tags(tweet_temp)\n",
    "   date[i] = date_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = pd.DataFrame([tweets, sentiment_label, emojis, hashtags, tag, likes, retweet, date]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = df_prepro.rename(columns = {0 : \"tweet\", 1: \"sentiment_label\", 2: 'emojis', 3: 'hashtags', 4: \"tags\", 5: \"likes\", 6: \"retweets\", 7: \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative', 'neutral', 'positive'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_prepro['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'positive': 67663, 'neutral': 65973, 'negative': 49734})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(list(df_prepro['sentiment_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the data set after deleating some repeated/incorrect entriees\n",
    "df_prepro.to_csv('Data/PreProData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
